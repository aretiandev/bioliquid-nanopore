{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanopore S3 Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3cmd get --recursive s3://aretian-genomics/nanopore/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull reference genome from S3\n",
    "!s3cmd get s3://aretian-genomics/nanopore/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index reference genome\n",
    "!samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select chromosomes or regions\n",
    "!samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna chr17 > chr17_selected.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chromosome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fasta files\n",
    "def read_fasta_genome(fasta_file,chromosome_header):\n",
    "    clean_data = fasta_file.read().replace(\"\\n\", \"\")\n",
    "    clean_data = clean_data.replace(chromosome_header,\"\") # get rid of header\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assembly-algorithm/reference-genome/chr17_selected.fa') as f:\n",
    "    chr17_genome = read_fasta_genome(f,'>chr17')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.bioinformatics.org/sms/iupac.html for IUPAC nucleotide codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: ['W', 'N', 'R', 'K', 'A', 'Y', 'T', 'G', 'S', 'C']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique characters: {list(set(chr17_genome))}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected chromosome from reference genome is 83257441 BP long\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selected chromosome from reference genome is {len(chr17_genome)} BP long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chromosome Selection (Nanopore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembly-algorithm\t\t step1.py\n",
      "bioliquid_chr17_pompe.bam\t step2.py\n",
      "bioliquid_chr17_pompe.bam.bai\t step3.py\n",
      "bioliquid_chr17_pompe_reads.txt  str_pipeline_download_data.ipynb\n",
      "bioliquid_chr17_pompe.txt\t str_pipeline_step2.sh\n",
      "chunk2.csv\t\t\t str_pipeline_step3.sh\n",
      "extracted_reads.txt\t\t temp\n",
      "families.csv\t\t\t Untitled.ipynb\n",
      "output.bam\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./assembly-algorithm/data/bioliquid_chr17_pompe_500reads.txt', delimiter = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.from_records(data, columns=['readID', 'unknown1', 'chromosome', 'start_index','sequence_description','unknown2','unknown3','unknown4','sequence','quality']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "reads = []\n",
    "with open('./assembly-algorithm/data/bioliquid_chr17_pompe_500reads.txt', newline = '') as games:                                                                                          \n",
    "    game_reader = csv.reader(games, delimiter='\\t')\n",
    "    \n",
    "    for game in game_reader:\n",
    "        time.sleep(1) # prevent memory read rate error\n",
    "        reads.append(game) \n",
    "        #print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanopore_reads = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fasta files and convert to pandas dataframe\n",
    "def read_fasta_nanopore(fasta_file,chromosome_header):\n",
    "    clean_data = fasta_file.read().replace(\"\\n\", \"\")\n",
    "    #clean_data = clean_data.replace(chromosome_header,\"\") # get rid of header\n",
    "\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assembly-algorithm/data/nanopore_chr17.fa') as f:\n",
    "    chr17_nanopore_reads = read_fasta_nanopore(f,'>chr17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanopore_chr17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull reads from chr17\n",
    "!samtools view bioliquid_chr17_pompe.bam | head -n 5000 > bioliquid_chr17_pompe_5000reads.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window Assembly Algorithm (V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pad left-right on reference genome\n",
    "- Create sliding window ~5000 BP\n",
    "    - Avg. read length ~23,452\n",
    "- Cluster on each window\n",
    "- Slide window + jump_length\n",
    "    - Jump length ~1000 BP\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding left-right on reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sliding window of length 5000 bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Perform clustering by positioning sliding window in position 0\n",
    "Move window to 0 + jump_length\n",
    "Jump_length: 1000\n",
    "Perform clustering again and repeat\n",
    "Chr17 total length: 80M. If jump_length=1000, total number of iterations = 80K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads = pd.read_csv('bioliquid_chr17_pompe_100reads.csv')\n",
    "reads = pd.read_csv('bioliquid_chr17_pompe_500reads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads['start_index'] = reads['unknown2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = reads[0:105]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in test.index:\n",
    "    print(f\"{test.loc[ID,'start_index']}, {len(test.loc[ID,'sequence'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = max(test['start_index']) + len(test.loc[test['start_index'].idxmax(),'sequence'])\n",
    "start_index = min(test['start_index'])\n",
    "index_range = end_index-start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_overlap = np.zeros((len(test),index_range)) # empty matrix for reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in test.index:\n",
    "    print(f\"Adding raad cover for read {ID+1}: Index-->{test.loc[ID,'start_index']}, length-->{len(test.loc[ID,'sequence'])}\")\n",
    "    start = test.loc[ID,'start_index']-start_index\n",
    "    #print(start)\n",
    "    end = start+len(test.loc[ID,'sequence'])\n",
    "    #print(end)\n",
    "    read_overlap[ID,start:end] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = read_overlap.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(summary,index=range(start_index,end_index), columns=['overlap_count'])\n",
    "df['position'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['position'],df['overlap_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_bound = 23000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = {}\n",
    "sequences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID in test.index:\n",
    "    sequence = list(test.loc[ID,'sequence']) # turn sequence string into list\n",
    "    \n",
    "    start = test.loc[ID,'start_index']-left_bound\n",
    "    \n",
    "    if start > 0: # sequence starts to the right of the bound\n",
    "        \n",
    "        sequence = (start*['X'])+sequence\n",
    "        sequence_trimmed = ''.join(sequence) # keep entire sequence\n",
    "        \n",
    "        \n",
    "    elif start < 0: # sequence starts to the left of the bound\n",
    "        sequence_trimmed = ''.join(sequence[np.abs(start):])\n",
    "        \n",
    "                                    \n",
    "    #sequences[test.loc[ID,'ID']] = sequence_trimmed\n",
    "    sequences.append(sequence_trimmed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_sequence_length = len(max(sequences, key=len)) # get longest sequence\n",
    "for i in range(len(sequences)):\n",
    "    pad_length = longest_sequence_length - len(sequences[i])\n",
    "    sequence = list(sequences[i])\n",
    "    sequence = sequence+(pad_length*['X'])\n",
    "    sequence = sequence[0:3000] # ADDED - RANGE from start index\n",
    "    sequence = ''.join(sequence)\n",
    "    sequences[i] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sequence_area_of_interest'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "NUCLEOTIDE_VOCABULARY = [\n",
    "    'A','C','G','T','X'\n",
    "]\n",
    "        \n",
    "def nucleotide_to_one_hot(nucleotide_sequence):\n",
    "    to_return = []\n",
    "    for char in nucleotide_sequence:\n",
    "        if char in NUCLEOTIDE_VOCABULARY:\n",
    "            to_append = np.zeros(len(NUCLEOTIDE_VOCABULARY))\n",
    "            to_append[NUCLEOTIDE_VOCABULARY.index(char)] = 1.\n",
    "            to_return.append(to_append)\n",
    "        else:\n",
    "            raise ValueError('Could not one-hot code character {}'.format(char))\n",
    "    return np.array(to_return)\n",
    "\n",
    "nucleotide_to_one_hot('GTCATACX') # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences:\n",
    "    sequences_test.append(list(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sequence_one_hot'] = test.sequence_area_of_interest.apply(lambda x: nucleotide_to_one_hot(x).flatten())  # Map variants that have insertions or deletions to all zeros (483 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "X = enc.fit_transform(sequences_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['PCA1'] = np.nan\n",
    "test['PCA2'] = np.nan\n",
    "for ID in test.index:\n",
    "    test.loc[ID,'PCA1'] = X_pca[ID][0].round(5)\n",
    "    test.loc[ID,'PCA2'] = X_pca[ID][1].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data=test,x='PCA1',y='PCA2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Standardizing the features\n",
    "X = test[['PCA1','PCA2']]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "distortions = []\n",
    "\n",
    "for k in range(1, 5):\n",
    "    KMeans_model = KMeans(n_clusters=k, random_state=42)\n",
    "    KMeans_model.fit(X)\n",
    "    distortions.append(KMeans_model.inertia_)\n",
    "\n",
    "plt.plot(range(1, 5), distortions,  marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for k in range(2, 5):\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(X)\n",
    "    score = silhouette_score(X, model.labels_)\n",
    "    print(\"Silhouette Score for k = \", k, \"is\", score)\n",
    "    silhouette_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 5), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer3 = KElbowVisualizer(KMeans(), k=(2,5))\n",
    "\n",
    "visualizer3.fit(X) # Fit the data to the visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, random_state=42)\n",
    "cls2 = model.fit(X)\n",
    "test['kmeans_cls2'] = cls2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(data=test,x='PCA1',y='PCA2',hue='kmeans_cls2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomics-local",
   "language": "python",
   "name": "genomics-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
